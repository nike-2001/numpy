{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of NumPy Coding Assignment 2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nike-2001/numpy/blob/main/Copy_of_NumPy_Coding_Assignment_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06f22e03-17d2-45c2-95a4-b617e45a6339"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6i9LXloxlIWr"
      },
      "source": [
        "### 1. Macro-averaged F1-score\n",
        "\n",
        "Implement the `macro_averaged_f1_score()` function which computes the Macro-averaged F1 score for a multi-class classification problem.\n",
        "\n",
        "\n",
        "$$\\text{Precision (P) = }\\frac{TP}{TP+FP}$$\n",
        "<br>\n",
        "$$\\text{Recall (R) = }\\frac{TP}{TP+FN}$$\n",
        "<br>\n",
        "$$\\text{F1-score = }\\frac{2PR}{P+R}$$\n",
        "<br>\n",
        "$$\\text{Macro-averaged F1-score = }\\frac{\\text{Sum of F1-scores of all classes }}{\\text{Number of classes}}$$\n",
        "\n",
        "\n",
        "**Arguments**:\n",
        "* **`actual_Y`** :  Actual labels of instances.\n",
        "    * A 1D numpy array of chars\n",
        "    * Array shape: (number of instances, )\n",
        "* **`predicted_Y`**: Predicted labels of instances.\n",
        "    * A 1D numpy array of chars\n",
        "    * Array shape: (number of instances, )\n",
        "    * Assume that `predicted_Y` does not have labels which are not in `actual_Y`\n",
        "\n",
        "\n",
        "**Returns**:\n",
        "Macro-averaged F1-score (A float value)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03aeced1-552d-46f5-910b-9f10ff01836f"
      },
      "source": [
        "def macro_averaged_f1_score(actual_Y, predicted_Y):\n",
        "  #ADD YOUR CODE HERE\n",
        "  unique_class_labels = np.unique(actual_Y)\n",
        "  total_f1_score = 0\n",
        "  for class_label in unique_class_labels:\n",
        "      predicted_positive_count = np.count_nonzero(predicted_Y == class_label)\n",
        "      actual_positive_count = np.count_nonzero(actual_Y == class_label)\n",
        "\n",
        "      true_positives = np.logical_and(actual_Y == class_label, predicted_Y == class_label)\n",
        "      true_positives_count = np.count_nonzero(true_positives)\n",
        "        \n",
        "      class_precision = 0.0\n",
        "      class_recall = 0.0\n",
        "      class_f1_score = 0.0\n",
        "\n",
        "      if(true_positives_count != 0):\n",
        "          class_precision = true_positives_count/predicted_positive_count\n",
        "          class_recall = true_positives_count/actual_positive_count\n",
        "          class_f1_score = (2*class_precision*class_recall)/(class_precision + class_recall)\n",
        "\n",
        "      total_f1_score += class_f1_score\n",
        "\n",
        "  return total_f1_score/unique_class_labels.shape[0]\n",
        "\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15FKlXP0n8MK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b3f372f-b138-4b77-8ffb-e02ff8402cd4"
      },
      "source": [
        "actual_Y = np.array([\"A\",\"A\",\"B\",\"C\",\"C\"])\n",
        "predicted_Y = np.array([\"A\",\"B\",\"B\",\"C\",\"B\"])\n",
        "f1_score = macro_averaged_f1_score(actual_Y, predicted_Y)\n",
        "print(np.round(f1_score, 3))\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.611\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iL-PvZB-pKpn"
      },
      "source": [
        "**Expected Output**:\n",
        "```\n",
        "0.611\n",
        "```"
      ]
    }
  ]
}